{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997259974479675},\n",
       " {'label': 'POSITIVE', 'score': 0.9998561143875122},\n",
       " {'label': 'POSITIVE', 'score': 0.9990635514259338},\n",
       " {'label': 'POSITIVE', 'score': 0.9264250993728638},\n",
       " {'label': 'NEGATIVE', 'score': 0.9988137483596802}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "        \"That wine is not great\",\n",
    "        \"The IPA is tasty\",\n",
    "        \"Subtle taste\",\n",
    "        \"Debatable taste\",\n",
    "        \"It could have been better\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(4, 16), dtype=int32, numpy=\n",
       "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
       "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
       "       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0],\n",
       "       [  101,  2008,  4511,  2003,  2025,  2307,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0],\n",
       "       [  101,  1996, 24531,  2003, 11937, 21756,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(4, 16), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "    \"That wine is not great\",\n",
    "    \"The IPA is tasty\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertModel: ['classifier', 'dropout_19', 'pre_classifier']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "model = TFAutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutput(last_hidden_state=<tf.Tensor: shape=(4, 16, 768), dtype=float32, numpy=\n",
       "array([[[-0.17978   ,  0.23332772,  0.63209945, ..., -0.30166644,\n",
       "          0.5008203 ,  0.14814392],\n",
       "        [ 0.27577662,  0.6497117 ,  0.31997734, ..., -0.07599552,\n",
       "          0.5136176 ,  0.13292289],\n",
       "        [ 0.9045852 ,  0.09851328,  0.29497236, ...,  0.33519474,\n",
       "         -0.14074147, -0.6464028 ],\n",
       "        ...,\n",
       "        [ 0.14658971,  0.5660597 ,  0.3235284 , ..., -0.3375748 ,\n",
       "          0.5099783 , -0.05610824],\n",
       "        [ 0.75000435,  0.04872589,  0.17380023, ...,  0.4684149 ,\n",
       "          0.00296685, -0.60837525],\n",
       "        [ 0.0519442 ,  0.37294796,  0.5223326 , ...,  0.35840523,\n",
       "          0.6500426 , -0.38829762]],\n",
       "\n",
       "       [[-0.293706  ,  0.72825605, -0.14972647, ..., -0.11868097,\n",
       "         -1.0226723 , -0.04215714],\n",
       "        [-0.22063547,  0.9383844 , -0.09512451, ..., -0.36431688,\n",
       "         -0.6605215 ,  0.24069746],\n",
       "        [-0.15360747,  0.89874995, -0.07276407, ..., -0.21891722,\n",
       "         -0.8527597 ,  0.07099392],\n",
       "        ...,\n",
       "        [-0.30174723,  0.9002206 , -0.01995054, ..., -0.10816893,\n",
       "         -0.8412146 , -0.08614336],\n",
       "        [-0.33384192,  0.96742475, -0.07294345, ..., -0.19517332,\n",
       "         -0.81813246, -0.06339093],\n",
       "        [-0.34538004,  0.88236165, -0.04263784, ..., -0.0992645 ,\n",
       "         -0.8328662 , -0.10647417]],\n",
       "\n",
       "       [[-0.93632567,  0.88163066,  0.15044488, ..., -0.15130211,\n",
       "         -0.68096447, -0.47151798],\n",
       "        [-1.1688721 ,  1.1283965 ,  0.1757986 , ..., -0.23593716,\n",
       "         -0.6815717 , -0.44200292],\n",
       "        [-0.7634696 ,  1.0940634 ,  0.03633566, ..., -0.36483136,\n",
       "         -0.64522076, -0.45698634],\n",
       "        ...,\n",
       "        [-0.87510264,  0.9285793 ,  0.13570902, ..., -0.10212138,\n",
       "         -0.6416471 , -0.43475643],\n",
       "        [-0.8931592 ,  0.9747921 ,  0.17094398, ..., -0.08013345,\n",
       "         -0.5828034 , -0.40972698],\n",
       "        [-0.9015384 ,  0.9373991 ,  0.12165275, ..., -0.09838162,\n",
       "         -0.63901705, -0.44877037]],\n",
       "\n",
       "       [[ 0.50208354, -0.0713145 ,  0.31475124, ...,  0.44566086,\n",
       "          0.8968262 , -0.3615731 ],\n",
       "        [ 0.7499468 , -0.1050311 ,  0.07124911, ...,  0.4720698 ,\n",
       "          0.9680656 , -0.39233267],\n",
       "        [ 0.59743166,  0.08885252,  0.434425  , ...,  0.27165622,\n",
       "          0.6934809 , -0.17411843],\n",
       "        ...,\n",
       "        [ 0.4075601 , -0.14818694,  0.1812459 , ...,  0.4723781 ,\n",
       "          0.8286435 , -0.22202663],\n",
       "        [ 0.39697757, -0.11028177,  0.20942464, ...,  0.436494  ,\n",
       "          0.7919066 , -0.2567454 ],\n",
       "        [ 0.46779   , -0.13545416,  0.13390253, ...,  0.56597924,\n",
       "          0.88866377, -0.2282304 ]]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(inputs)\n",
    "outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_77']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[-1.5606976,  1.6122824],\n",
       "       [ 4.1692314, -3.3464472],\n",
       "       [ 4.545752 , -3.6566067],\n",
       "       [-4.2676663,  4.5786138]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[4.0195297e-02, 9.5980471e-01],\n",
       "       [9.9945587e-01, 5.4418424e-04],\n",
       "       [9.9972600e-01, 2.7393154e-04],\n",
       "       [1.4389539e-04, 9.9985611e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1060cd3e306bdb917693ad15b950c3c971d6daea3be81f3e1761505107e734f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
