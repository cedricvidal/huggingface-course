{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = TFBertModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.25.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading pre trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertModel at 0x291ef4640>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
       "array([[ 101, 7592,  999,  102],\n",
       "       [ 101, 4658, 1012,  102],\n",
       "       [ 101, 3835,  999,  102]], dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model_inputs = tf.constant(encoded_sequences)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(3, 4, 768), dtype=float32, numpy=\n",
       "array([[[ 4.4495693e-01,  4.8276237e-01,  2.7797231e-01, ...,\n",
       "         -5.4032277e-02,  3.9393473e-01, -9.4769850e-02],\n",
       "        [ 2.4942882e-01, -4.4092998e-01,  8.1772339e-01, ...,\n",
       "         -3.1916562e-01,  2.2992229e-01, -4.1171607e-02],\n",
       "        [ 1.3667561e-01,  2.2517815e-01,  1.4502043e-01, ...,\n",
       "         -4.6914484e-02,  2.8224206e-01,  7.5565889e-02],\n",
       "        [ 1.1788861e+00,  1.6738467e-01, -1.8187001e-01, ...,\n",
       "          2.4671446e-01,  1.0440780e+00, -6.1970316e-03]],\n",
       "\n",
       "       [[ 3.6435831e-01,  3.2464463e-02,  2.0257670e-01, ...,\n",
       "          6.0110882e-02,  3.2451272e-01, -2.0995531e-02],\n",
       "        [ 7.1865952e-01, -4.8725182e-01,  5.1740390e-01, ...,\n",
       "         -4.4011989e-01,  1.4553049e-01, -3.7544839e-02],\n",
       "        [ 3.3223230e-01, -2.3270883e-01,  9.4875634e-02, ...,\n",
       "         -2.5268146e-01,  3.2171986e-01,  8.1131514e-04],\n",
       "        [ 1.2523218e+00,  3.5754350e-01, -5.1320642e-02, ...,\n",
       "         -3.7839732e-01,  1.0526476e+00, -5.6254840e-01]],\n",
       "\n",
       "       [[ 2.4042311e-01,  1.4717817e-01,  1.2110332e-01, ...,\n",
       "          7.6061599e-02,  3.3564439e-01,  2.8261662e-01],\n",
       "        [ 6.5700626e-01, -3.2786548e-01,  2.4967623e-01, ...,\n",
       "         -2.5919527e-01,  2.0174684e-01,  3.3275110e-01],\n",
       "        [ 2.0159549e-01,  1.5782690e-01,  9.8975105e-03, ...,\n",
       "         -3.8850495e-01,  4.1307458e-01,  3.9731991e-01],\n",
       "        [ 1.0174990e+00,  6.4386624e-01, -7.8146589e-01, ...,\n",
       "         -4.2109242e-01,  1.0925055e+00, -4.8456501e-02]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\n",
       "array([[-0.6856277 ,  0.5262491 ,  0.99995273, ...,  0.9999873 ,\n",
       "        -0.6112341 ,  0.99706566],\n",
       "       [-0.60545826,  0.49971703,  0.99981904, ...,  0.9999407 ,\n",
       "        -0.6753277 ,  0.97692657],\n",
       "       [-0.77015066,  0.5447416 ,  0.99994177, ...,  0.9999845 ,\n",
       "        -0.46549398,  0.98939013]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(model_inputs)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1060cd3e306bdb917693ad15b950c3c971d6daea3be81f3e1761505107e734f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
